# -*- encoding: utf-8 -*-
# @Author: SWHL
# @Contact: liekkaskono@163.com
import importlib
import time
from typing import Dict
import streamlit as st

from utils import logger, make_prompt, read_yaml

config = read_yaml("config.yaml")

st.set_page_config(
    page_title=config.get("title"),
    page_icon=":robot:",
)

def predict_only_model(text, model):
    # params_dict = st.session_state["params"]
    response = model(text, history=None,)
    bot_print(response)


def bot_print(content, avatar: str = "ğŸ¤–"):
    with st.chat_message("assistant", avatar=avatar):
        message_placeholder = st.empty()
        full_response = ""
        for chunk in content.split():
            full_response += chunk + " "
            time.sleep(0.05)
            message_placeholder.markdown(full_response + "â–Œ")
        message_placeholder.markdown(full_response)




def tips(txt: str, wait_time: int = 2, icon: str = "ğŸ‰"):
    st.toast(txt, icon=icon)
    time.sleep(wait_time)


if __name__ == "__main__":
    title = config.get("title")
    version = config.get("version", "0.0.1")
    st.markdown(
        f"<h3 style='text-align: center;'>{title} v{version}</h3><br/>",
        unsafe_allow_html=True,
    )

    llm_module = importlib.import_module("llm")
    llm_params: Dict[str, Dict] = config.get("LLM_API")

    menu_col1, menu_col2, menu_col3 = st.columns([1, 1, 1])
    select_model = menu_col1.selectbox("ğŸ¨LLM:", llm_params.keys())
    if "ERNIEBot" in select_model:
        with st.expander("LLM ErnieBot", expanded=True):
            opt_col1, opt_col2 = st.columns([1, 1])
            api_type = opt_col1.selectbox(
                "API Type(å¿…é€‰)",
                options=["aistudio", "qianfan", "yinian"],
                help="æä¾›å¯¹è¯èƒ½åŠ›çš„åç«¯å¹³å°",
            )
            access_token = opt_col2.text_input(
                "Access Token(å¿…å¡«) &nbsp;[å¦‚ä½•è·å¾—ï¼Ÿ](https://github.com/PaddlePaddle/ERNIE-Bot-SDK/blob/2520b8d482a36e39fcb2287614197a981ecb6b79/erniebot/docs/authentication.md)",
                "",
                help="ç”¨äºè®¿é—®åç«¯å¹³å°çš„access tokenï¼ˆå‚è€ƒä½¿ç”¨è¯´æ˜è·å–ï¼‰ï¼Œå¦‚æœè®¾ç½®äº†AKã€SKåˆ™æ— éœ€è®¾ç½®æ­¤å‚æ•°",
            )
            llm_params[select_model]["api_type"] = api_type

            if access_token:
                llm_params[select_model]["access_token"] = access_token

    MODEL_OPTIONS = {
        name: getattr(llm_module, name)(**params) for name, params in llm_params.items()
    }



    with st.expander("ğŸ’¡Prompt", expanded=False):
        text_area = st.empty()
        input_prompt = text_area.text_area(
            label="Input",
            max_chars=500,
            height=200,
            label_visibility="hidden",
            value=config.get("DEFAULT_PROMPT"),
            key="input_prompt",
        )

    input_txt = st.chat_input("é—®ç‚¹å•¥å§ï¼")
    if input_txt:
        with st.chat_message("user", avatar="ğŸ˜€"):
            st.markdown(input_txt)

        llm = MODEL_OPTIONS[select_model]

        if not input_prompt:
            input_prompt = config.get("DEFAULT_PROMPT")

        bot_print("ä»çŸ¥è¯†åº“ä¸­æŠ½å–ç»“æœä¸ºç©ºï¼Œç›´æ¥é‡‡ç”¨LLMçš„æœ¬èº«èƒ½åŠ›å›ç­”ã€‚", avatar="ğŸ“„")
        predict_only_model(input_txt, llm)